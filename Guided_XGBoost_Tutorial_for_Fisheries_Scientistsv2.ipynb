{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VHSajna/0xfolio/blob/master/Guided_XGBoost_Tutorial_for_Fisheries_Scientistsv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install copernicusmarine\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import earthaccess\n",
        "from match_func import match_nearest\n",
        "import netCDF4\n",
        "import h5netcdf\n",
        "import math\n",
        "import copernicusmarine\n",
        "import geopandas as gpd\n",
        "import earthaccess\n",
        "import xarray as xr\n",
        "import cartopy.feature as cfeature\n",
        "from shapely.ops import unary_union\n",
        "from tqdm import tqdm\n",
        "import cmocean\n",
        "auth = earthaccess.login(persist=True)\n",
        "\n",
        "#os.chdir(r'/home/jovyan/Hackweek2025/proj_2025_sdm/data')\n",
        "#trawl_df = pd.read_csv('fisheries_with_pace_rrs_avw2.csv') #read data\n",
        "trawl_df = pd.read_csv('/home/jovyan/proj_2025_sdm/data/fisheries_with_pace_rrs_avw2.csv') #read data\n",
        "# --- Data Cleaning (Crucial Step!) ---\n",
        "\n",
        "# 1. Ensure the date column is a proper datetime object.\n",
        "#    This is vital for matching with the environmental data.\n",
        "trawl_df['TOWDATETIME_EST'] = pd.to_datetime(trawl_df['TOWDATETIME_EST'])\n",
        "\n",
        "# 2. Ensure coordinates are numeric\n",
        "trawl_df['LON'] = pd.to_numeric(trawl_df['LON'])\n",
        "trawl_df['LAT'] = pd.to_numeric(trawl_df['LAT'])\n",
        "\n",
        "# Display the first few rows and data types to verify\n",
        "print(\"Catch Data Head:\")\n",
        "print(trawl_df.head())\n",
        "print(\"\\nData Types:\")\n",
        "print(trawl_df.info())\n",
        "\n",
        "# --- Standardize Column Names to Match Xarray Dims ---\n",
        "# This is the crucial step to ensure congruency.\n",
        "rename_dict = {\n",
        "    'TOWDATETIME_EST': 'time',\n",
        "    'LAT': 'latitude',\n",
        "    'LON': 'longitude'\n",
        "}\n",
        "trawl_df = trawl_df.rename(columns=rename_dict)\n",
        "\n",
        "matched=[]\n",
        "for item in trawl_df.columns:\n",
        "    if 'Rrs' in item:  # Using the 'in' operator for substring check\n",
        "        pass\n",
        "    else:\n",
        "        matched.append(item)\n",
        "\n",
        "sub = trawl_df[list(matched)] #subset (remove RRS columns)\n",
        "sub['station'] = np.arange(1,len(sub)+1) #add station col\n",
        "\n",
        "tot_sum = {}\n",
        "for s in sub.columns[5:-2]:\n",
        "   tot_sum.update({s:int(sub[s].sum())})  #get total sum of all values\n",
        "\n",
        "sorted_tot_sum = sorted(tot_sum.items(), key=lambda item: item[1],reverse=True) #sort by max-->min\n",
        "\n",
        "\n",
        "abu=[]\n",
        "for val in sorted_tot_sum:\n",
        "    if val[1]>50000: #if more than 50000 observations\n",
        "        abu.append(val[0])\n",
        "\n",
        "\n",
        "#plot max valeus\n",
        "for s in sub[abu].columns:\n",
        "    plt.plot(sub.station, sub[s],label=s)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', )\n",
        "plt.xlabel('Station #')\n",
        "plt.ylabel('Total # of obs')\n",
        "plt.title('Species with more than 50000 observations')\n",
        "plt.show()\n",
        "\n",
        "min_lat = trawl_df['latitude'].min()\n",
        "max_lat = trawl_df['latitude'].max()\n",
        "min_lon = trawl_df['longitude'].min()\n",
        "max_lon = trawl_df['longitude'].max()\n",
        "\n",
        "# Print DataFrame shape and columns (for trawl_df)\n",
        "print(\"\\n--- Trawl Data Shape and Columns ---\")\n",
        "print(\"Trawl DataFrame Shape:\", trawl_df.shape)\n",
        "#print(\"Trawl DataFrame Columns:\", trawl_df.columns.tolist())\n",
        "\n",
        "\n",
        "print(f\"Trawl data latitude range: {min_lat} to {max_lat}\")\n",
        "print(f\"Trawl data longitude range: {min_lon} to {max_lon}\")\n",
        "print(\"Trawl Data Time Range:\", trawl_df[\"time\"].min(), \"to\", trawl_df[\"time\"].max())\n",
        "\n",
        "# Create a figure and a 3x2 grid of subplots\n",
        "bathym = cfeature.NaturalEarthFeature(name='bathymetry_K_200', scale='10m', category='physical')\n",
        "bathym = unary_union(list(bathym.geometries()))\n",
        "fig, axs = plt.subplots(2, 3, figsize=(12, 8),subplot_kw={'projection': cartopy.crs.PlateCarree()})\n",
        "\n",
        "# Flatten the axs array for easy iteration if needed, though direct indexing works too\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Example data for plotting\n",
        "x = np.linspace(0, 10, 100)\n",
        "\n",
        "# Loop through each subplot and plot data\n",
        "for i, ax in enumerate(axs):\n",
        "    ax.scatter(sub[sub[sub[abu].columns[i]]==0].longitude, sub[sub[sub[abu].columns[i]]==0].latitude,c='r',s=8,label='Absence')\n",
        "    ax.scatter(sub[sub[sub[abu].columns[i]]!=0].longitude, sub[sub[sub[abu].columns[i]]!=0].latitude,c='g',s=8,label='Presence')\n",
        "    ax.add_feature(cartopy.feature.COASTLINE, linewidth=1) #add coastlines\n",
        "    ax.add_feature(cartopy.feature.LAND,  facecolor='lightgrey') #add land mask\n",
        "    ax.add_geometries(bathym, facecolor='none', edgecolor='black', crs=cartopy.crs.PlateCarree()) #add bathymetry line\n",
        "    ax.add_feature(cfeature.OCEAN, facecolor='azure')\n",
        "    ax.set_title(sub[abu].columns[i])\n",
        "    ax.legend()\n",
        "fig.suptitle('Species presence by station')\n",
        "\n",
        "def get_pace_path(trawl_df, short_name):\n",
        "    tspan = (trawl_df.date.min(), trawl_df.date.max())\n",
        "    bbox = (-76.75, 33, -63, 46) #west,south,east,north\n",
        "    #clouds = (0, 50)\n",
        "    results = earthaccess.search_data(\n",
        "        short_name=short_name,\n",
        "        temporal=tspan,\n",
        "        bounding_box=bbox,\n",
        "        granule_name=\"*.8*.4km*\")\n",
        "        #cloud_cover=clouds,\n",
        "    paths = earthaccess.open(results)\n",
        "    return paths\n",
        "\n",
        "\n",
        "def open_pace(paths):\n",
        "    d8=[]\n",
        "    for file in paths:\n",
        "        d = xr.open_dataset(file)\n",
        "        d8.append(d.attrs['time_coverage_start'])\n",
        "    ds = xr.open_mfdataset(paths, combine='nested',concat_dim='datetime').assign_coords({'time':d8}) #add new dimension and assign time data to it\n",
        "    ds = ds.rename({'datetime':'time'}) #rename to time\n",
        "    ds = ds.where((ds.lat > 34.40918) & (ds.lat < 46.362305) & (-63>ds.lon) & (-77< ds.lon),drop=True) #general spatial subset\n",
        "    ds = ds.rename({'lat':'latitude','lon':'longitude'})\n",
        "    #ds['time']=[pd.to_datetime(d) for d in ds.time.values] #convert to pandas datetime\n",
        "    ds['time'] = pd.to_datetime(ds.time) #convert to pandas datetime\n",
        "    return ds\n",
        "\n",
        "#trawl_df pre-processing\n",
        "trawl_df['date'] = [str(d).split(' ')[0] for d in trawl_df.time]\n",
        "#trawl_df['time'] = [pd.to_datetime(d.replace(' ','T')) for d in trawl_df.TOWDATETIME_EST] #format as timestamp\n",
        "\n",
        "#get pace data\n",
        "paths_avw= get_pace_path(trawl_df,\"PACE_OCI_L3M_AVW\")\n",
        "paths_chl= get_pace_path(trawl_df,\"PACE_OCI_L3M_CHL\")\n",
        "paths_kd= get_pace_path(trawl_df,\"PACE_OCI_L3M_KD\")\n",
        "\n",
        "\n",
        "ds_avw, ds_chl = list(map(open_pace,[paths_avw, paths_chl])) #get path values for each variable\n",
        "\n",
        "#match data\n",
        "data=[ds_avw, ds_chl] #list of datasets\n",
        "var_names =['avw','chlor_a'] #list of variable names\n",
        "for d in range(len(data)):\n",
        "    if d == 0:\n",
        "        trawl_df = match_nearest(trawl_df,data[d],var_names[d],var_names[d],date=trawl_df.time) #get full trawl_df + avw\n",
        "    else:\n",
        "        trawl_df[var_names[d]]=match_nearest(trawl_df,data[d],var_names[d],var_names[d],date=trawl_df.time)[var_names[d]] #add chl in\n",
        "\n",
        "# Check for any missing values, which could indicate a mismatch\n",
        "#print(\"\\nMissing values count:\")\n",
        "#print(trawl_df[['avw','chlor_a'].isnull().sum())\n",
        "\n",
        "bathym = cfeature.NaturalEarthFeature(name='bathymetry_K_200', scale='10m', category='physical')\n",
        "bathym = unary_union(list(bathym.geometries()))\n",
        "\n",
        "for i in range(2):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6),subplot_kw={'projection': cartopy.crs.PlateCarree()})\n",
        "\n",
        "    im=axs[0].pcolormesh(ds_avw.longitude, ds_avw.latitude, ds_avw.avw[i],cmap=cmocean.cm.solar)\n",
        "    axs[0].add_feature(cartopy.feature.COASTLINE, linewidth=1) #add coastlines\n",
        "    axs[0].add_feature(cartopy.feature.LAND,  facecolor='lightgrey') #add land mask\n",
        "    axs[0].add_geometries(bathym, facecolor='none', edgecolor='black', crs=cartopy.crs.PlateCarree()) #add bathymetry line\n",
        "    axs[0].set_title('Apparent visible wavelength')\n",
        "\n",
        "    im1=axs[1].pcolormesh(ds_chl.longitude, ds_chl.latitude, ds_chl.chlor_a[i],cmap=cmocean.cm.algae)\n",
        "    axs[1].add_feature(cartopy.feature.COASTLINE, linewidth=1) #add coastlines\n",
        "    axs[1].add_feature(cartopy.feature.LAND,  facecolor='lightgrey') #add land mask\n",
        "    axs[1].add_geometries(bathym, facecolor='none', edgecolor='black', crs=cartopy.crs.PlateCarree()) #add bathymetry line\n",
        "    axs[1].set_title('Chlorophyll')\n",
        "\n",
        "    fig.colorbar(im, ax=axs[0],shrink=0.65)\n",
        "    fig.colorbar(im1, ax=axs[1],shrink=0.65)\n",
        "    fig.suptitle('PACE Variables: '+ str(ds_avw.time[i].values).split('T')[0],y=0.85)\n",
        "    fig.show()\n",
        "\n",
        "isna= [math.isnan(d) for d in trawl_df.avw]\n",
        "fig = plt.figure(figsize=(10, 7)) #set figure size\n",
        "map_projection = cartopy.crs.PlateCarree() #set map projection\n",
        "ax = plt.axes(projection=map_projection)\n",
        "plt.pcolormesh(ds_chl.longitude, ds_chl.latitude, ds_chl.chlor_a[0],cmap=cmocean.cm.algae)\n",
        "plt.scatter(trawl_df[isna][:20].longitude, trawl_df[isna][:20].latitude,s=8,c='r')\n",
        "ax.add_feature(cartopy.feature.COASTLINE, linewidth=1) #add coastlines\n",
        "ax.add_feature(cartopy.feature.LAND,  facecolor='lightgrey') #add land mask\n",
        "ax.add_geometries(bathym, facecolor='none', edgecolor='black', crs=cartopy.crs.PlateCarree()) #add bathymetry line\n",
        "plt.title('8-Day chlor_a \\n Visualize missing data after matchup')\n",
        "\n",
        "def get_pace_nan_replace(trawl_df, short_name):\n",
        "    #used to address NaN values to allow for more robust data\n",
        "    tspan = (trawl_df.date.min(), trawl_df.date.max())\n",
        "    bbox = (-76.75, 33, -63, 46) #west,south,east,north\n",
        "    #clouds = (0, 50)\n",
        "    results = earthaccess.search_data(\n",
        "        short_name=short_name,\n",
        "        temporal=tspan,\n",
        "        bounding_box=bbox,\n",
        "        granule_name=\"*.M*.4km*\") #update to month\n",
        "        #cloud_cover=clouds,\n",
        "    paths = earthaccess.open(results)\n",
        "    return paths\n",
        "\n",
        "def extract_scalar(val):\n",
        "    #since avw values are objects, this function extracts the\n",
        "    #actual, usable scalar value (a float)\n",
        "    if isinstance(val, (list, np.ndarray)): #checks if instance of a list or a NumPy list\n",
        "        if np.size(val)== 1:\n",
        "            return val[0] if isinstance(val, list) else val.item()\n",
        "        else:\n",
        "            return np.nan\n",
        "    elif hasattr(val, 'item'): #unwraps from NumPy object, xarray scalar DataArray, etc.\n",
        "        try:\n",
        "            return val.item()\n",
        "        except:\n",
        "            return np.nan\n",
        "    return val\n",
        "\n",
        "#converting datatype to usable float64\n",
        "trawl_df['avw'] = trawl_df['avw'].apply(extract_scalar)\n",
        "trawl_df['chlor_a'] = trawl_df['chlor_a'].apply(extract_scalar)\n",
        "\n",
        "trawl_df['avw'] = pd.to_numeric(trawl_df['avw'], errors='coerce') #another conversion to float\n",
        "trawl_df['chlor_a'] = pd.to_numeric(trawl_df['chlor_a'], errors='coerce') #another conversion to float\n",
        "\n",
        "#creating list of indices containing NaN values\n",
        "na_index_avw = trawl_df[trawl_df.avw.isna()].index\n",
        "na_index_chl = trawl_df[trawl_df.chlor_a.isna()].index\n",
        "\n",
        "#creates dataset containing only NaN values\n",
        "trawl_df_avw_nan_only = trawl_df.loc[na_index_avw]\n",
        "trawl_df_chl_nan_only = trawl_df.loc[na_index_chl]\n",
        "\n",
        "trawl_df_avw_nan_only = trawl_df_avw_nan_only.drop('avw', axis = 1)\n",
        "trawl_df_chl_nan_only = trawl_df_chl_nan_only.drop('chlor_a', axis = 1)\n",
        "\n",
        "nan_avw_path = get_pace_nan_replace(trawl_df,\"PACE_OCI_L3M_AVW\")\n",
        "nan_chl_path = get_pace_nan_replace(trawl_df,\"PACE_OCI_L3M_CHL\")\n",
        "\n",
        "nan_avw_ds, nan_chl_ds = list(map(open_pace,[nan_avw_path, nan_chl_path])) #get path values for each variable\n",
        "\n",
        "matched_monthly = match_nearest(trawl_df_avw_nan_only.reset_index(), nan_avw_ds, 'avw', 'avw', date=trawl_df_avw_nan_only.time)\n",
        "trawl_df.loc[na_index_avw, 'avw'] = matched_monthly['avw']\n",
        "\n",
        "matched_monthly = match_nearest(trawl_df_chl_nan_only.reset_index(), nan_chl_ds, 'chlor_a', 'chlor_a', date=trawl_df_chl_nan_only.time)\n",
        "trawl_df.loc[na_index_chl, 'chlor_a'] = matched_monthly['chlor_a']\n",
        "\n",
        "glorys_ds= copernicusmarine.open_dataset(dataset_id = 'cmems_mod_glo_phy_myint_0.083deg_P1D-m',minimum_longitude=-77, maximum_longitude=-63,minimum_latitude=34,maximum_latitude=46,)\n",
        "glorys_subset = glorys_ds.where((glorys_ds['time.year'] > 2023) & (glorys_ds['time.month']>2)& (glorys_ds['time.month']<6), drop=True)\n",
        "glorys_subset['time'] = [pd.Timestamp(d) for d in glorys_subset.time.values]\n",
        "\n",
        "# Select four time slices (modify indices if needed)\n",
        "time_indices = [0, int(len(glorys_subset.time)/3), int(2*len(glorys_subset.time)/3), -1]\n",
        "\n",
        "# Set up figure\n",
        "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
        "\n",
        "for i, t in enumerate(time_indices):\n",
        "    # Plot\n",
        "    ax = axes[0, i]\n",
        "    glorys_subset.bottomT.isel(time=t).plot(ax=ax, cmap=\"viridis\")\n",
        "    ax.set_title(f\"Bottom Temperature - Time {t}\")\n",
        "\n",
        "    # Plot 2\n",
        "    ax = axes[1, i]\n",
        "    glorys_subset.mlotst.isel(time=t).plot(ax=ax, cmap=\"plasma\")\n",
        "    ax.set_title(f\"Mixed Layer Thickness - Time {t}\")\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Vectorized Extraction ---\n",
        "\n",
        "# 1. Create DataArrays for coordinates from your standardized catch_df.\n",
        "#    The names 'time', 'latitude', 'longitude' now match the env_ds dimensions.\n",
        "times = xr.DataArray(trawl_df.time.values, dims=\"observation\")\n",
        "lats = xr.DataArray(trawl_df.latitude.values, dims=\"observation\")\n",
        "lons = xr.DataArray(trawl_df.longitude.values, dims=\"observation\")\n",
        "\n",
        "# 2. Extract the 3D variables (bottomT, mlotst) in one go.\n",
        "#    These variables don't have a 'depth' dimension.\n",
        "extracted_3d_data = glorys_subset[['bottomT', 'mlotst']].sel(\n",
        "    time=times,\n",
        "    latitude=lats,\n",
        "    longitude=lons,\n",
        "    method=\"nearest\"\n",
        ")\n",
        "\n",
        "# 3. Handle the 4D variable (so) separately to deal with depth.\n",
        "#    Since it's bottom trawl data, we select the deepest available grid cell.\n",
        "#    .isel(depth=-1) selects the last element along the depth dimension.\n",
        "extracted_so = glorys_subset['so'].sel(\n",
        "    time=times,\n",
        "    latitude=lats,\n",
        "    longitude=lons,\n",
        "    method=\"nearest\"\n",
        ").isel(depth=-1) # Select the bottom-most depth layer\n",
        "\n",
        "# --- Merge results back into your original DataFrame ---\n",
        "\n",
        "# Add the extracted 3D variables\n",
        "trawl_df['bottom_temp'] = extracted_3d_data['bottomT'].values\n",
        "trawl_df['mld'] = extracted_3d_data['mlotst'].values\n",
        "\n",
        "# Add the extracted bottom salinity\n",
        "trawl_df['bottom_salinity'] = extracted_so.values\n",
        "\n",
        "# --- Display the final merged DataFrame ---\n",
        "print(\"\\n--- FINAL MERGED DATA ---\")\n",
        "# Displaying relevant columns for verification\n",
        "final_columns = [\n",
        "    'time', 'latitude', 'longitude', 'MEAN_DEPTH',\n",
        "    'bottom_temp', 'mld', 'bottom_salinity'\n",
        "]\n",
        "print(trawl_df[final_columns].head())\n",
        "\n",
        "# Check for any missing values, which could indicate a mismatch\n",
        "print(\"\\nMissing values count:\")\n",
        "print(trawl_df[final_columns].isnull().sum())\n",
        "\n",
        "bathym = cfeature.NaturalEarthFeature(name='bathymetry_K_200', scale='10m', category='physical')\n",
        "bathym = unary_union(list(bathym.geometries()))\n",
        "fig, axs = plt.subplots(2, 3, figsize=(12, 6),subplot_kw={'projection': cartopy.crs.PlateCarree()})\n",
        "\n",
        "# Flatten the axs array for easy iteration if needed, though direct indexing works too\n",
        "axs = axs.flatten()\n",
        "\n",
        "var=['mld','bottom_temp','bottom_salinity','chlor_a','avw','Rrs_brightness']\n",
        "var_n = ['Mixed layer depth','Bottom Temperature','Bottom Salinity','Chlorophyll a','AVW','RRS brightness']\n",
        "# Loop through each subplot and plot data\n",
        "for i, ax in enumerate(axs):\n",
        "    if var[i] == 'bottom_temp':\n",
        "        cmap = cmocean.cm.thermal\n",
        "    elif var[i] =='chlor_a':\n",
        "        cmap=cmocean.cm.algae\n",
        "    elif var[i] == 'Rrs_brightness':\n",
        "        cmap=cmocean.cm.solar\n",
        "    else:\n",
        "        cmap = cmocean.cm.deep\n",
        "    im=ax.scatter(trawl_df.longitude, trawl_df.latitude, c=trawl_df[var[i]], label=var[i],s=8, cmap=cmap,vmin=trawl_df[var[i]].min(),vmax=trawl_df[var[i]].max(), )\n",
        "    ax.add_feature(cartopy.feature.COASTLINE, linewidth=1) #add coastlines\n",
        "    ax.add_feature(cartopy.feature.LAND,  facecolor='lightgrey') #add land mask\n",
        "    ax.add_geometries(bathym, facecolor='none', edgecolor='black', crs=cartopy.crs.PlateCarree()) #add bathymetry line\n",
        "    ax.add_feature(cfeature.OCEAN, facecolor='azure')\n",
        "    fig.colorbar(im, ax=ax,shrink=0.95)\n",
        "    ax.set_title(var_n[i])\n",
        "fig.suptitle('Matched variables')\n",
        "\n",
        "trawl_df['chlor_a'] = [extract_scalar(d) for d in trawl_df.chlor_a] #change dtype from object to scalar\n",
        "trawl_df['avw'] = [extract_scalar(d) for d in trawl_df.avw] #change dtype from object to scalar\n",
        "\n",
        "\n",
        "cpue_butterfish = trawl_df['butterfish']/trawl_df['SWEPT_AREA_km']\n",
        "cpue_squid = trawl_df['longfin squid']/trawl_df['SWEPT_AREA_km']\n",
        "cpue_hake = trawl_df['silver hake']/trawl_df['SWEPT_AREA_km']\n",
        "\n",
        "trawl_df['chlora_log10']=[np.log10(d) for d in trawl_df.chlor_a]\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "df = trawl_df.copy()\n",
        "print(df)\n",
        "\n",
        "# --- Define your target (y) and predictor (X) variables ---\n",
        "TARGET_SPECIES = ['butterfish', 'silver hake', 'longfin squid', 'spot']\n",
        "\n",
        "\n",
        "# Exclude species, location, and metadata columns.\n",
        "PREDICTOR_COLUMNS = ['avw', 'chlor_a', 'MEAN_DEPTH', 'bottom_temp', 'mld', 'rrs_brightness']\n",
        "\n",
        "y = np.log1p(df[TARGET_SPECIES])\n",
        "X_df = df[PREDICTOR_COLUMNS]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\n--- Step 2: Initializing the XGBoost Model ---\")\n",
        "\n",
        "# Let's define the core parameters.\n",
        "xgb_reg = xgb.XGBRegressor(\n",
        "    # --- Boosting Parameters (How the model learns) ---\n",
        "    n_estimators=1000,         # Analogous to the \"richness\" of the model. Number of trees to build.\n",
        "                               # We set this high and use early stopping to find the optimal number.\n",
        "    learning_rate=0.05,        # Analogous to a shrinkage parameter. Lower values make the model more robust.\n",
        "\n",
        "    # --- Tree Complexity Parameters ---\n",
        "    max_depth=5,               # Analogous to `k` in s(x, k=...). Controls max interaction depth.\n",
        "    min_child_weight=1,        # A form of regularization. Prevents learning highly specific patterns.\n",
        "    gamma=0.1,                 # Analogous to `sp`. A value > 0 penalizes splits, making the model more conservative.\n",
        "    subsample=0.8,             # Use 80% of data for building each tree. Adds randomness to fight overfitting.\n",
        "    colsample_bytree=0.8,      # Use 80% of features for building each tree. Also for overfitting.\n",
        "\n",
        "    # --- Regularization Parameters ---\n",
        "    reg_alpha=0.005,           # L1 regularization on leaf weights.\n",
        "    reg_lambda=1,              # L2 regularization on leaf weights.\n",
        "\n",
        "    # --- Technical Parameters ---\n",
        "    objective='reg:squarederror', # The loss function to optimize.\n",
        "    n_jobs=-1,                 # Use all available CPU cores.\n",
        "    random_state=42,           # For reproducibility.\n",
        "    eval_metric='rmse'         # Metric to monitor during training.\n",
        ")\n",
        "\n",
        "print(\"Model initialized with parameters:\")\n",
        "print(xgb_reg.get_params())\n",
        "\n",
        "xgb_reg.fit(X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False) # Set verbose=True to see the training progress\n",
        "\n",
        "print(f\"Model training complete.\")\n",
        "\n",
        "results = xgb_reg.evals_result()\n",
        "best_iteration = np.argmin(results['validation_0']['rmse'])\n",
        "best_score = results['validation_0']['rmse'][best_iteration]\n",
        "\n",
        "print(f\"Best iteration found: {best_iteration}\")\n",
        "print(f\"Best validation RMSE: {best_score:.4f}\")\n",
        "\n",
        "print(\"\\n--- Step 4: Making Predictions ---\")\n",
        "predictions = xgb_reg.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "rmse = np.sqrt(mean_squared_error(y_val, predictions))\n",
        "print(f\"Final RMSE on validation data: {rmse:.4f}\")\n",
        "\n",
        "# --- Step 5: Model Interpretation (The `mgcv::plot.gam` Analogy) ---\n",
        "\n",
        "\n",
        "print(\"\\n--- Step 5: Interpreting the Model ---\")\n",
        "\n",
        "# 5a. Feature Importance\n",
        "print(\"Plotting feature importance...\")\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "xgb.plot_importance(xgb_reg, ax=ax, max_num_features=10, height=0.8, title=\"Feature Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "import shap\n",
        "\n",
        "# 5b. SHAP Values\n",
        "print(\"\\nCalculating and plotting SHAP values...\")\n",
        "# Create a SHAP explainer object\n",
        "explainer = shap.TreeExplainer(xgb_reg)\n",
        "\n",
        "# Calculate SHAP values for the validation set\n",
        "\n",
        "values = explainer.shap_values(X_val)\n",
        "\n",
        "# Summary plot: Shows the distribution of impacts for each feature.\n",
        "# Red means high feature value, blue means low.\n",
        "# Points to the right increase the prediction, points to the left decrease it.\n",
        "shap.summary_plot(shap_values, X_val, plot_type=\"dot\", show=False)\n",
        "plt.title(\"SHAP Summary Plot for Fisheries Data\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "# --- Step 9: Create Partial Dependence Plots ---\n",
        "print(f\"\\n--- Creating Partial Dependence Plots for {TARGET_SPECIES} ---\")\n",
        "\n",
        "# The PDP shows the marginal effect one or two features have on the predicted outcome.\n",
        "# It's the closest equivalent to the `plot.gam()` smooths.\n",
        "for predictor in PREDICTOR_COLUMNS:\n",
        "    if predictor in X_train.columns:\n",
        "        try:\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            PartialDependenceDisplay.from_estimator(\n",
        "                estimator=xgb_reg,\n",
        "                X=X_train,\n",
        "                features=[predictor],\n",
        "                ax=ax\n",
        "            )\n",
        "            ax.set_title(f\"Partial Dependence Plot for {predictor}\\n({TARGET_SPECIES})\")\n",
        "            ax.set_ylabel(\"Partial Dependence (log CPUE scale)\")\n",
        "            plt.grid(True, linestyle='--', alpha=0.6)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create PDP for {predictor}. Error: {e}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "V8EqdbLt3vNk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}